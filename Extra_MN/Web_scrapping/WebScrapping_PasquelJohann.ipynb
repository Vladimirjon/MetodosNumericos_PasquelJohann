{
 "cells": [
  {
   "cell_type": "raw",
   "id": "03b04dcc-a64f-4d8b-98fe-a7c4b6571aab",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Web Scrapping\"\n",
    "author: \"Pasquel Johann\"\n",
    "lang: es\n",
    "format:\n",
    "  pdf:\n",
    "    toc: true\n",
    "    toc-title: \"Tabla de Contenidos\"\n",
    "execute:\n",
    "  echo: true\n",
    "  error: false\n",
    "  warning: false\n",
    "  jupyter: python3.11\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6298f-6ff9-494d-96cd-de5a2f5c557c",
   "metadata": {},
   "source": [
    "# GITHUB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3e1aa-426f-46a0-822e-5ab118d7a9ed",
   "metadata": {},
   "source": [
    "https://github.com/Vladimirjon/MetodosNumericos_PasquelJohann/tree/main/Extra_MN/Web_scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d7e3cb-cba8-4991-a3a0-f3fb23a20595",
   "metadata": {},
   "source": [
    "# 1. OBJETIVOS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43b09c-6b5f-47a1-9821-88c762d1da0d",
   "metadata": {},
   "source": [
    "* Investigar y comprender el funcionamiento de las librerías **requests** y **BeautifulSoup** para realizar solicitudes HTTP y analizar documentos HTML en el contexto de web scraping.\n",
    "* Desarrollar un script en Python que implemente ambas librerías para extraer información estructurada, como títulos, encabezados y enlaces, desde una página web seleccionada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53fdb01-bd73-44ce-9133-04fb13f1d486",
   "metadata": {},
   "source": [
    "# 2. INTRODUCCIÓN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44468071-1e16-4601-a7fb-0d923ed63b6c",
   "metadata": {},
   "source": [
    "El ***web scraping*** es una técnica que permite extraer datos de sitios web de manera automatizada. En un mundo impulsado por la información, esta práctica ha ganado relevancia en diversos campos, como el análisis de datos, el comercio electrónico, las investigaciones académicas y la minería de información. \n",
    "\n",
    "Herramientas como Python, con su ecosistema de bibliotecas especializadas, han hecho que el web scraping sea más accesible y eficiente para desarrolladores y analistas.\n",
    "\n",
    "**PROCESO DE WEB SCRAPING**\n",
    "\n",
    "**1. Estructura del sitio Web**\n",
    "   \n",
    "Analizar la estructura del sitio web objetivo. Esto implica inspeccionar el código HTML del sitio, identificando las etiquetas y atributos relevantes para los datos que se desean extraer. \n",
    "\n",
    "**2. Configuración del Entorno**\n",
    "\n",
    "Configurar un entorno de desarrollo que incluya las herramientas adecuadas. Librerías adecuadas y dependencias.\n",
    "\n",
    "**3. Implementación del Scraping**\n",
    "\n",
    "- Realizar una solicitud HTTP al servidor para obtener el HTML de la página.\n",
    "- Analizar el HTML recibido\n",
    "- Extraer los datos deseados\n",
    "\n",
    "El web scraping es una herramienta poderosa que transforma el acceso y análisis de datos en procesos automatizados y eficientes. A través de Python, los desarrolladores pueden construir soluciones personalizadas para extraer información valiosa de sitios web, respetando siempre las normativas legales y éticas. Al combinar las mejores prácticas y las herramientas adecuadas, el web scraping se convierte en un recurso indispensable para el manejo de datos en el mundo actual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fbad9d-a609-47eb-9421-e0a25ab0f8da",
   "metadata": {},
   "source": [
    "# 3. Código en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d42a9c-9502-4c7c-a215-5a2146364513",
   "metadata": {},
   "source": [
    "## 3.1 Configuración del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ca063a-fe9c-46a5-818a-5d9c8a6acce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/johann/miniforge3/envs/vladimirjon/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/johann/miniforge3/envs/vladimirjon/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/johann/miniforge3/envs/vladimirjon/lib/python3.11/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/johann/miniforge3/envs/vladimirjon/lib/python3.11/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/johann/miniforge3/envs/vladimirjon/lib/python3.11/site-packages (from requests) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8b0f5a4-d309-4270-ae81-298788840425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /home/johann/miniforge3/envs/vladimirjon/lib/python3.11/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/johann/miniforge3/envs/vladimirjon/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff32de9-097b-43ff-bbf7-ab57ea89661f",
   "metadata": {},
   "source": [
    "## 3.2 Implementación en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e82c2-8863-443c-b002-4f0dba0000eb",
   "metadata": {},
   "source": [
    "### Importación de Librerías\n",
    "\n",
    "En esta sección se importan las bibliotecas necesarias para realizar web scraping, analizar los datos obtenidos y almacenarlos de forma estructurada en un archivo CSV:\n",
    "- **`requests`**: Permite realizar solicitudes HTTP.\n",
    "- **`BeautifulSoup`**: Facilita el análisis del contenido HTML.\n",
    "- **`csv`**: Proporciona herramientas para manipular y guardar datos en formato CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21307822-81b8-4b6e-abdf-4ffad320932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d6091-b0a6-42be-befd-4d9438e5539c",
   "metadata": {},
   "source": [
    "### Obtención del Contenido HTML\n",
    "\n",
    "En esta sección, se realiza una solicitud HTTP al servidor utilizando la biblioteca `requests`.\n",
    "\n",
    "Esto permite obtener el código fuente HTML de la página web objetivo, que será analizado en pasos posteriores. El contenido es almacenado en la variable `response` para su procesamiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a757d7-73c2-464e-afcc-94b4631ed349",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.scrapingbee.com/blog/web-scraping-101-with-python/\"\n",
    "response = req.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afef6ec8-d3ea-43d7-923b-ace9abbdec15",
   "metadata": {},
   "source": [
    "### Extracción y Almacenamiento de Datos\n",
    "\n",
    "Análisis del contenido HTML previamente obtenido para extraer información estructurada, como el título principal de la página, encabezados y enlaces. Finalmente, los datos son almacenados en un archivo CSV para su posterior análisis.\n",
    "\n",
    "#### Resultado esperado:\n",
    "- Un archivo llamado `scraped_data.csv` con el siguiente contenido estructurado:\n",
    "  - Título principal de la página.\n",
    "  - Lista de encabezados encontrados.\n",
    "  - Lista de enlaces (hasta 10) con sus respectivas URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ba696ba-70bd-4768-ba72-69a0f837971e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título principal: Python Web Scraping: Full Tutorial With Examples (2024)\n",
      "\n",
      "Datos guardados en scraped_data.csv\n"
     ]
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    main_title = soup.find('h1').text\n",
    "    print(f\"Título principal: {main_title}\\n\")\n",
    "\n",
    "    headers = soup.find_all(['h2', 'h3'])\n",
    "    links = soup.find_all('a', href=True)\n",
    "\n",
    "    # Guardar en un archivo CSV\n",
    "    with open('scraped_data.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Tipo', 'Contenido'])\n",
    "\n",
    "        writer.writerow(['Título principal', main_title])\n",
    "        writer.writerow(['Encabezados'])\n",
    "        for header in headers:\n",
    "            writer.writerow(['Encabezado', header.text])\n",
    "\n",
    "\n",
    "        writer.writerow(['Enlaces'])\n",
    "        for link in links[:10]:  \n",
    "            writer.writerow(['Enlace', link['href']])\n",
    "\n",
    "    print(\"Datos guardados en scraped_data.csv\")\n",
    "else:\n",
    "    print(f\"Error al acceder a la página: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc34fbf6-55a3-47f9-a962-d210889b77e2",
   "metadata": {},
   "source": [
    "## 4. CONCLUSIONES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff8cfb-14a1-4cd3-842c-244dc39d82c0",
   "metadata": {},
   "source": [
    "* El uso combinado de **requests** y **BeautifulSoup** facilita el proceso de web scraping al permitir realizar solicitudes HTTP eficientes y analizar documentos HTML de manera estructurada.\n",
    "* El script en Python permitió validar la capacidad de estas librerías para extraer información significativa, como títulos, encabezados y enlaces, y almacenar los resultados en un formato reutilizable como CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305792a-4343-44e5-8352-d1c37ade5366",
   "metadata": {},
   "source": [
    "## 5. REFERENCIAS BIBLIOGRÁFICAS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209d92ac-f012-4275-9a5f-11fa3f5f9ee0",
   "metadata": {},
   "source": [
    "https://www.scrapingbee.com/blog/web-scraping-with-scrapy/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba675b2-dbf9-4386-9008-b5236deb63c9",
   "metadata": {},
   "source": [
    "https://www.scrapingbee.com/blog/web-scraping-101-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149dfd68-c6bd-474b-a9b9-855d1d30a103",
   "metadata": {},
   "source": [
    "https://www.scrapingbee.com/blog/web-scraping-without-getting-blocked/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
